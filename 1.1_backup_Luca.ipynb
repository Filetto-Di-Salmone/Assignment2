{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUI QUELLO SERIO ZI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 1.1: Log-likelihood and Gradient ascend Rule *(3/10 Points)*** \n",
    "\n",
    "Write the likelihood $L(\\theta)$ and log-likelihood $l(\\theta)$ of the parameters $\\theta$.\n",
    "\n",
    "Recall the probabilistic interpretation of the hypothesis $h_\\theta(x)= P(y=1|x;\\theta)$ and that $h_\\theta(x)=\\frac{1}{1+\\exp(-\\theta^T x)}$.\n",
    "\n",
    "Also derive the gradient $\\frac{\\delta l(\\theta)}{\\delta \\theta_j}$ of $l(\\theta)$ and write the gradient update equation.\n",
    "\n",
    "* **Likelihood**: $L(\\theta)$\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "L(\\theta)=P(y \\mid X ; \\theta)=\\prod_{i=1}^m P\\left(y^i \\mid x^{(i)} ; \\theta\\right)=\\prod_{i=1}^m h\\left(x^{(i)}\\right)^{y^{(i)}}\\left(1-h_\\theta x^{(i)}\\right)^{1-y^{(i)}}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "To find the maximum we compute the log-likelihood because it is more convenient to work with the log-likelihood function in maximum likelihood estimation. Logarithms are strictly increasing functions, so maximizing the likelihood is equivalent to maximizing the log-likelihood.\n",
    "* **Log-Likelihood**: $l(\\theta)$\n",
    "\\begin{equation*}\n",
    "l(\\theta)=\\log L(\\theta)=\\sum_{i=1}^m y^{(i)} \\log h_\\theta\\left(x^{(i)}\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-h_\\theta\\left(x^{(i)}\\right)\\right)\n",
    "\\end{equation*}\n",
    "* **Hypothesis**:\n",
    "$h_\\theta(x)= P(y=1|x;\\theta)$ and $h_\\theta(x)=\\frac{1}{1+\\exp(-\\theta^T x)}$.\n",
    "\n",
    "Which means\n",
    "\\begin{equation*}\n",
    "P(y=1|X = x;\\theta) = g(\\theta^T x)^1 [1-g(\\theta^T x)]^{1-1} = g(\\theta^T x)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "P(y=0|X = x;\\theta) = g(\\theta^T x)^0 [1-g(\\theta^T x)]^{1-0} = 1-g(\\theta^T x)\n",
    "\\end{equation*}\n",
    "\n",
    "If we suppose $ p = g(\\theta^T x) $, we can see it as a Bernoullian distribution,\n",
    "\\begin{equation*}\n",
    "Y \\sim Ber(p)\n",
    "\\end{equation*}\n",
    "hence:\n",
    "\\begin{equation*}\n",
    "P(y|x;\\theta) = h_\\theta (x)^y (1-h_\\theta(x))^{1-y}\n",
    "\\end{equation*}\n",
    "\n",
    "We can map predicted values to probabilities using the **Sigmoid function** (also known as logistic function). The function maps any real value into another value between 0 and 1. If we consider the sigmoid function, defined by\n",
    "\\begin{equation*}\n",
    "g(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{equation*}\n",
    "and we apply  to the **hypothesis**, we obtain\n",
    "\\begin{equation*}\n",
    "h_\\theta(x)= g(\\theta^T x) = \\frac{1}{1+e^{-\\theta^T x}}\n",
    "\\end{equation*}\n",
    "* **Gradient of log-likelihood**: \n",
    "\n",
    "\\begin{gathered}\n",
    "\\frac{\\partial}{\\partial\\theta_j}l(\\theta)=\\sum_{i=1}^m \\frac{\\partial}{\\partial \\theta_j} y^{(i)} \\ln \\left(g\\left(\\theta^T x^{(i)}\\right)\\right)+\\frac{\\partial}{\\partial \\theta_j}\\left(1-y^{(i)}\\right) \\ln \\left(1-g\\left(\\theta^T x^{(i)}\\right)\\right)= \\\\\n",
    "\\quad=\\sum_{i=1}^m\\left[\\frac{y^{(i)}}{g\\left(\\theta^T x^{(i)}\\right)}-\\frac{1-y^{(i)}}{1-g\\left(\\theta^T x^{(i)}\\right)}\\right] \\frac{\\delta}{\\delta \\theta_j} g\\left(\\theta^T x^{(i)}\\right)= \\\\\n",
    "=\\sum_{i=1}^m\\left[\\frac{y^{(i)}}{g\\left(\\theta^T x^{(i)}\\right)}-\\frac{1-y^{(i)}}{1-g\\left(\\theta^T x^{(i)}\\right)}\\right] g\\left(\\theta^T x^{(i)}\\right)\\left[1-g\\left(\\theta^T x\\right)\\right] x_j^{(i)}= \\\\\n",
    "=\\sum_{i=1}^m\\left[\\frac{y^{(i)}-g\\left(\\theta^T x^{(i)}\\right)}{g\\left(\\theta^T x^{(i)}\\right)\\left[1-g\\left(\\theta^T x^{(i)}\\right)\\right]}\\right] g\\left(\\theta^T x^{(i)}\\right)\\left[1-g\\left(\\theta^T x^{(i)}\\right)\\right] x_j^{(i)}= \\\\\n",
    "=\\sum_{i=1}^m\\left[y^{(i)}-g\\left(\\theta^T x^{(i)}\\right)\\right] x_j^{(i)}\n",
    "\\end{gathered}\n",
    "* *Update equation* for the **gradient ascent**:\n",
    "\n",
    "With the log-likelihood we can calculate the gradient ascent\n",
    "\\begin{equation*}\n",
    "\\theta_j=\\theta_j + \\alpha\\frac{\\partial}{\\partial\\theta_j}l(\\theta)\n",
    "\\end{equation*}\n",
    "The idea behind **gradient ascent** is that if you continuously take small steps in the direction of your gradient, you will eventually make it to a local maximum.\n",
    "\n",
    "Hence:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta_j=\\theta_j + \\alpha\\sum_{i=1}^m[y^{(i)}-g(\\theta^T x^{(i)}) ]x^{(i)}_j\n",
    "\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
